# Streaming Query API Response Documentation

## Endpoint
**POST** `/chat/query/stream`

## Overview
This endpoint provides **Server-Sent Events (SSE)** streaming responses for ultra-fast, real-time chat interactions optimized for web chatbots.

---

## Request Format

### HTTP Method
`POST`

### Headers
```http
Content-Type: application/json
```

### Request Body
```json
{
  "user_id": "string",
  "team_id": "string", 
  "session_id": "string",
  "question": "string",
  "top_k": 10  // Optional, defaults to 10
}
```

### Required Fields
- `user_id` - Unique identifier for the user
- `team_id` - Team/organization identifier
- `session_id` - Current conversation session ID
- `question` - User's query/question

### Optional Fields
- `top_k` - Number of relevant document chunks to retrieve (default: 10)

---

## Response Format

### Response Type
**Server-Sent Events (SSE)** - `text/event-stream`

### Response Headers
```http
Content-Type: text/event-stream; charset=utf-8
Cache-Control: no-cache, no-store, must-revalidate, no-transform
X-Accel-Buffering: no
Connection: keep-alive
Transfer-Encoding: chunked
```

---

## Event Stream Format

The API streams multiple events in **SSE format**. Each event follows this structure:

```
data: <JSON_OBJECT>\n\n
```

### Event Types

The response includes **4 different event types** streamed in sequence:

---

## 1. Metadata Event

**Sent first** - Contains processing information and debug data.

### Format
```
data: {"type": "metadata", "content": {...}}\n\n
```

### Content Structure
```json
{
  "type": "metadata",
  "content": {
    "detected_language": {
      "detected_language": "English",
      "language_code": "en",
      "language_switch_requested": false,
      "target_language": null,
      "target_language_code": null
    },
    "clarified_query": "What are the key features of the product?",
    "timing": {
      "language_detection": 0.245,
      "query_clarification": 0.312,
      "total_parallel": 0.356
    },
    "chunks_count": 8,
    "chunks_summary": [
      {
        "filename": "product_guide.pdf",
        "score": 0.892,
        "text_preview": "The product features include advanced analytics, real-time monitoring..."
      }
    ]
  }
}
```

### Metadata Fields Explained

| Field | Type | Description |
|-------|------|-------------|
| `detected_language` | Object | Language detection results |
| `detected_language.detected_language` | String | Human-readable language name |
| `detected_language.language_code` | String | ISO language code (e.g., "en", "hi", "hi-en") |
| `detected_language.language_switch_requested` | Boolean | Whether user requested language change |
| `clarified_query` | String | Processed/clarified version of user's question |
| `timing` | Object | Performance metrics in seconds |
| `chunks_count` | Integer | Number of relevant documents retrieved |
| `chunks_summary` | Array | Preview of retrieved document chunks |

---

## 2. Response Chunk Events

**Streamed continuously** - Contains partial response text as it's generated by the LLM.

### Format
```
data: {"type": "response_chunk", "content": "..."}\n\n
```

### Example Stream
```
data: {"type": "response_chunk", "content": "The"}\n\n
data: {"type": "response_chunk", "content": " key"}\n\n
data: {"type": "response_chunk", "content": " features"}\n\n
data: {"type": "response_chunk", "content": " include"}\n\n
data: {"type": "response_chunk", "content": ":\n\n"}\n\n
data: {"type": "response_chunk", "content": "1. Advanced"}\n\n
data: {"type": "response_chunk", "content": " analytics"}\n\n
```

### Content Structure
```json
{
  "type": "response_chunk",
  "content": "partial text chunk"
}
```

**Note:** Multiple `response_chunk` events are sent in rapid succession to build the complete response.

---

## 3. Suggested Questions Event

**Sent after response completes** - Contains follow-up questions.

### Format
```
data: {"type": "suggested_questions", "content": [...]}\n\n
```

### Content Structure
```json
{
  "type": "suggested_questions",
  "content": [
    "What are the pricing options?",
    "How do I install the product?",
    "What technical support is available?",
    "Can I integrate it with existing systems?"
  ]
}
```

### Fields Explained

| Field | Type | Description |
|-------|------|-------------|
| `type` | String | Always "suggested_questions" |
| `content` | Array[String] | List of 3-5 follow-up questions |

**Features:**
- Generated in parallel with response streaming for zero latency
- Questions are contextually relevant to the current conversation
- All questions can be answered from available document contexts
- Generated in the same language as the response

---

## 4. Done Event

**Sent last** - Signals end of stream.

### Format
```
data: {"type": "done"}\n\n
```

### Content Structure
```json
{
  "type": "done"
}
```

This event indicates the stream has completed successfully.

---

## Error Event

**Sent if an error occurs** during processing.

### Format
```
data: {"type": "error", "content": "error message"}\n\n
```

### Content Structure
```json
{
  "type": "error",
  "content": "Error message describing what went wrong"
}
```

---

## Complete Response Flow Example

Here's a **complete sequence** of events you'll receive:

```
data: {"type": "metadata", "content": {"detected_language": {"detected_language": "English", "language_code": "en", "language_switch_requested": false}, "clarified_query": "What are system requirements?", "timing": {"language_detection": 0.234, "query_clarification": 0.289, "total_parallel": 0.312}, "chunks_count": 5, "chunks_summary": [{"filename": "system_guide.pdf", "score": 0.876, "text_preview": "System requirements..."}]}}\n\n

data: {"type": "response_chunk", "content": "##"}\n\n

data: {"type": "response_chunk", "content": " System"}\n\n

data: {"type": "response_chunk", "content": " Requirements"}\n\n

data: {"type": "response_chunk", "content": "\n\n"}\n\n

data: {"type": "response_chunk", "content": "The"}\n\n

data: {"type": "response_chunk", "content": " minimum"}\n\n

data: {"type": "response_chunk", "content": " system"}\n\n

data: {"type": "response_chunk", "content": " requirements"}\n\n

data: {"type": "response_chunk", "content": " are"}\n\n

data: {"type": "response_chunk", "content": ":\n\n"}\n\n

data: {"type": "response_chunk", "content": "- "}\n\n

data: {"type": "response_chunk", "content": "Operating"}\n\n

data: {"type": "response_chunk", "content": " System"}\n\n

data: {"type": "response_chunk", "content": ":"}\n\n

data: {"type": "response_chunk", "content": " Windows"}\n\n

data: {"type": "response_chunk", "content": " 10"}\n\n

data: {"type": "response_chunk", "content": " or"}\n\n

data: {"type": "response_chunk", "content": " higher"}\n\n

data: {"type": "response_chunk", "content": "\n"}\n\n

data: {"type": "response_chunk", "content": "- "}\n\n

data: {"type": "response_chunk", "content": "RAM"}\n\n

data: {"type": "response_chunk", "content": ":"}\n\n

data: {"type": "response_chunk", "content": " 8"}\n\n

data: {"type": "response_chunk", "content": "GB"}\n\n

data: {"type": "suggested_questions", "content": ["What are the recommended specifications?", "Is Linux supported?", "How much disk space is required?"]}\n\n

data: {"type": "done"}\n\n
```

---

## Client Implementation Example

### JavaScript (EventSource)

```javascript
const eventSource = new EventSource('/chat/query/stream', {
  method: 'POST',
  headers: {
    'Content-Type': 'application/json',
  },
  body: JSON.stringify({
    user_id: 'user123',
    team_id: 'team456',
    session_id: 'session789',
    question: 'What are the key features?'
  })
});

let fullResponse = '';
let metadata = null;
let suggestedQuestions = [];

eventSource.onmessage = function(event) {
  const data = JSON.parse(event.data);
  
  switch(data.type) {
    case 'metadata':
      metadata = data.content;
      console.log('Language:', metadata.detected_language.detected_language);
      console.log('Chunks found:', metadata.chunks_count);
      break;
      
    case 'response_chunk':
      fullResponse += data.content;
      // Display chunk immediately to user
      displayChunk(data.content);
      break;
      
    case 'suggested_questions':
      suggestedQuestions = data.content;
      // Display suggested questions
      displaySuggestions(suggestedQuestions);
      break;
      
    case 'done':
      console.log('Stream complete');
      eventSource.close();
      break;
      
    case 'error':
      console.error('Error:', data.content);
      eventSource.close();
      break;
  }
};

eventSource.onerror = function(error) {
  console.error('EventSource error:', error);
  eventSource.close();
};
```

### Python (httpx)

```python
import httpx
import json

url = 'http://localhost:8000/chat/query/stream'
payload = {
    'user_id': 'user123',
    'team_id': 'team456',
    'session_id': 'session789',
    'question': 'What are the key features?'
}

with httpx.stream('POST', url, json=payload) as response:
    full_response = ''
    
    for line in response.iter_lines():
        if line.startswith('data: '):
            data = json.loads(line[6:])
            
            if data['type'] == 'metadata':
                print(f"Language: {data['content']['detected_language']['detected_language']}")
                print(f"Chunks: {data['content']['chunks_count']}")
                
            elif data['type'] == 'response_chunk':
                full_response += data['content']
                print(data['content'], end='', flush=True)
                
            elif data['type'] == 'suggested_questions':
                print(f"\n\nSuggested questions: {data['content']}")
                
            elif data['type'] == 'done':
                print('\n\nStream complete')
                break
                
            elif data['type'] == 'error':
                print(f'\n\nError: {data["content"]}')
                break
```

---

## HTTP Status Codes

| Code | Description |
|------|-------------|
| `200` | Success - Stream starts |
| `400` | Bad Request - Missing required fields |
| `429` | Too Many Requests - Rate limit exceeded |
| `500` | Internal Server Error |

---

## Rate Limiting

The API implements **multi-layer rate limiting**:

- **Burst protection**: Prevents rapid successive requests
- **Hourly limits**: Caps requests per hour per session
- **Daily limits**: Maximum daily requests per session
- **IP-based limiting**: Prevents abuse from single IP
- **Concurrent request prevention**: One active request per session

### Rate Limit Headers (if applicable)
```http
X-RateLimit-Remaining: 45
X-RateLimit-Reset: 1635789600
```

---

## Performance Characteristics

### Speed Optimizations
- **Zero buffering** at all layers
- **Synchronous generator** (no async overhead)
- **Chunked transfer encoding** enabled
- **Immediate token delivery** from LLM to frontend
- **Parallel processing** of language detection and query clarification

### Expected Latency
- **First metadata event**: ~300-400ms
- **First response token**: ~400-600ms  
- **Suggested questions**: Sent in parallel (zero additional latency)
- **Token streaming rate**: Real-time LLM token generation speed

---

## Language Support

The API supports **multilingual queries** including:

- English (`en`)
- Hindi (`hi`)
- Hinglish (`hi-en`) - Hindi-English code-mixed
- Spanish (`es`)
- And more...

### Language Detection Features
- Automatic language detection from query
- Cross-language query support
- Language switching on user request
- Response in user's preferred language

---

## Advanced Features

### 1. Chat History Integration
- Maintains conversation context across session
- Handles unlimited follow-up questions
- References previous exchanges

### 2. Document Retrieval
- Retrieves top-k most relevant document chunks
- Semantic search using vector embeddings
- Relevance scoring and ranking

### 3. Smart Query Clarification
- Resolves ambiguous queries using chat history
- Handles follow-up questions intelligently
- Expands abbreviations and references

### 4. Suggested Questions
- Generated in parallel (no latency impact)
- Contextually relevant to conversation
- All answerable from available documents
- Language-aware generation

---

## Debugging & Monitoring

### Metadata Event Usage
The metadata event provides rich debugging information:

```json
{
  "type": "metadata",
  "content": {
    "detected_language": {...},     // Language detection results
    "clarified_query": "...",       // How query was interpreted
    "timing": {...},                // Performance metrics
    "chunks_count": 8,              // Documents found
    "chunks_summary": [...]         // Preview of sources
  }
}
```

**Use this to:**
- Verify correct language detection
- Check query interpretation
- Monitor retrieval quality
- Diagnose slow responses
- Validate document relevance

---

## Common Issues & Solutions

### Issue 1: No Response Streaming
**Symptom:** Entire response arrives at once instead of streaming

**Solution:** 
- Ensure client properly handles SSE format
- Check that proxy/load balancer doesn't buffer responses
- Verify `X-Accel-Buffering: no` header is respected

### Issue 2: Missing Suggested Questions
**Symptom:** Stream completes without suggested questions

**Possible Causes:**
- Suggested questions generation timed out (>5s)
- Error in question generation (check logs)
- No relevant contexts found

**Solution:**
- This is gracefully handled - response still works
- Check server logs for errors

### Issue 3: Rate Limit Errors (429)
**Symptom:** `429 Too Many Requests` error

**Solution:**
- Implement exponential backoff
- Use different `session_id` for concurrent users
- Contact admin to increase limits if needed

---

## Security Considerations

### Rate Limiting
- Prevents denial-of-service attacks
- Protects against resource exhaustion
- Session-based and IP-based controls

### Input Validation
- All required fields are validated
- Malformed requests return 400 error
- SQL injection protection via parameterized queries

### Session Management
- Session locks prevent concurrent abuse
- Automatic session release on completion/error
- Session cleanup after timeout

---

## Best Practices

### For Frontend Developers

1. **Handle all event types**
   ```javascript
   switch(data.type) {
     case 'metadata': // Store for debugging
     case 'response_chunk': // Display immediately
     case 'suggested_questions': // Show to user
     case 'done': // Clean up
     case 'error': // Handle gracefully
   }
   ```

2. **Display chunks immediately**
   - Don't wait to accumulate chunks
   - Update UI with each chunk for real-time feel

3. **Show suggested questions**
   - Make them clickable
   - Pre-fill input field on click

4. **Handle errors gracefully**
   - Show user-friendly error messages
   - Implement retry with exponential backoff
   - Log errors for debugging

5. **Use metadata for UX**
   - Show "Searching X documents..."
   - Display detected language
   - Indicate processing time

### For Backend Integration

1. **Reuse connections**
   - Use connection pooling
   - Keep event source alive

2. **Implement timeouts**
   - Set reasonable timeout (30-60s)
   - Handle timeout gracefully

3. **Log strategically**
   - Log request start/end
   - Log errors with context
   - Monitor performance metrics

---

## API Versioning

**Current Version:** v1

The endpoint may evolve. Breaking changes will be announced with new version endpoints.

---

## Support & Contact

For issues, questions, or feature requests:
- Check server logs for detailed error information
- Review this documentation thoroughly
- Contact development team for API-specific issues

---

## Changelog

### Current Version
- ✅ SSE streaming with zero buffering
- ✅ Multi-layer rate limiting
- ✅ Multilingual support (including Hinglish)
- ✅ Parallel processing optimizations
- ✅ Suggested questions generation
- ✅ Rich metadata event
- ✅ Smart query clarification
- ✅ Chat history integration

---

## Summary

This API provides **ultra-fast streaming responses** for conversational AI applications with:

- **Real-time streaming** via Server-Sent Events
- **Zero buffering** for immediate token delivery  
- **Rich metadata** for debugging and UX
- **Suggested questions** generated in parallel
- **Multi-language support** with automatic detection
- **Rate limiting** for security and stability

The streaming format allows building responsive, real-time chat interfaces with minimal latency and excellent user experience.
